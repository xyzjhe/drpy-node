import subprocess
import sqlite3
import json
import os
import sys
import time
import argparse
import threading
import queue
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# --- ç”¨æˆ·é…ç½®åŒºåŸŸ (User Configuration) ---
# é»˜è®¤ä½¿ç”¨çš„PHPçˆ¬è™«æ–‡ä»¶è·¯å¾„
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# è·å–é¡¹ç›®æ ¹ç›®å½• (å‡è®¾è„šæœ¬åœ¨ scripts/pythonï¼Œæ ¹ç›®å½•åœ¨ ../../)
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, "../../"))
DEFAULT_SPIDER = os.path.join(SCRIPT_DIR, "74Pç¦åˆ©å›¾ áµˆá¶»[ç”»].php")

# æ¯ä¸ªåˆ†ç±»é»˜è®¤æœ€å¤§çˆ¬å–é¡µæ•° (è®¾ç½®ä¸º 0 æˆ– None è¡¨ç¤ºä¸é™åˆ¶ï¼Œç›´åˆ°çˆ¬å®Œ)
DEFAULT_MAX_PAGES = 1
# é»˜è®¤å¹¶å‘çº¿ç¨‹æ•°
DEFAULT_THREADS = 8
# æ˜¯å¦è§£ææœ€ç»ˆæ’­æ”¾åœ°å€ (True: è§£æå¹¶å­˜å…¥resolved_url, False: åªå­˜å…¥åŸå§‹é“¾æ¥)
RESOLVE_FINAL_URLS = True
# PHP å‘½ä»¤è·¯å¾„
PHP_CMD = "php"
# æ¡¥æ¥è„šæœ¬è·¯å¾„
BRIDGE_SCRIPT = os.path.join(SCRIPT_DIR, "_crawler_bridge.php")

# --- æ•°æ®åº“ç®¡ç† (Database Manager) ---
class DBManager:
    def __init__(self, db_path):
        # check_same_thread=False å…è®¸åœ¨å¤šçº¿ç¨‹ä¸­ä½¿ç”¨åŒä¸€ä¸ªè¿æ¥ï¼Œä½†éœ€è¦æˆ‘ä»¬è‡ªå·±åŠ é”
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()
        self.lock = threading.Lock()
        self.init_tables()
        self._source_cache = {}

    def init_tables(self):
        with self.lock:
            # ä¼˜åŒ–ï¼šç§»é™¤ source_file å­—æ®µ (å‡è®¾æ¯ä¸ªDBåªå¯¹åº”ä¸€ä¸ªæº)
            # ä¼˜åŒ–ï¼šç§»é™¤ type_name (é€šè¿‡å…³è”æŸ¥è¯¢è·å–)
            # ä¼˜åŒ–ï¼šcrawled_at ä½¿ç”¨ INTEGER æ—¶é—´æˆ³
            
            self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS categories (
                tid TEXT PRIMARY KEY,
                name TEXT
            )
            ''')
            
            self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS vods (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vod_id TEXT UNIQUE,
                vod_name TEXT,
                type_id TEXT,
                vod_pic TEXT,
                vod_remarks TEXT,
                vod_content TEXT,
                crawled_at INTEGER,
                FOREIGN KEY(type_id) REFERENCES categories(tid)
            )
            ''')
            
            # æ–°å¢ï¼šæ’­æ”¾æºè¡¨ (å½’ä¸€åŒ– play_from)
            self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS play_sources (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE
            )
            ''')

            self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS episodes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vod_pk INTEGER,
                sid INTEGER,
                name TEXT,
                raw_url TEXT,
                resolved_url TEXT,
                FOREIGN KEY(vod_pk) REFERENCES vods(id),
                FOREIGN KEY(sid) REFERENCES play_sources(id)
            )
            ''')
            self.conn.commit()

    def get_or_create_source(self, name):
        # ç®€å•ç¼“å­˜
        if name in self._source_cache:
            return self._source_cache[name]
            
        with self.lock:
            try:
                self.cursor.execute('INSERT OR IGNORE INTO play_sources (name) VALUES (?)', (name,))
                self.cursor.execute('SELECT id FROM play_sources WHERE name = ?', (name,))
                row = self.cursor.fetchone()
                if row:
                    sid = row[0]
                    self._source_cache[name] = sid
                    return sid
                return 0
            except Exception as e:
                print(f"[DB Error] get_or_create_source: {e}")
                return 0

    def save_category(self, tid, name):
        with self.lock:
            try:
                self.cursor.execute('INSERT OR IGNORE INTO categories (tid, name) VALUES (?, ?)', 
                                    (tid, name))
                # å¦‚æœåç§°æ›´æ–°äº†ï¼Œä¹Ÿå¯ä»¥ update
                self.cursor.execute('UPDATE categories SET name = ? WHERE tid = ? AND name != ?', (name, tid, name))
                self.conn.commit()
            except Exception as e:
                print(f"[DB Error] save_category: {e}")

    def item_exists(self, vod_id):
        with self.lock:
            try:
                self.cursor.execute('SELECT 1 FROM vods WHERE vod_id = ?', (vod_id,))
                return self.cursor.fetchone() is not None
            except Exception as e:
                print(f"[DB Error] item_exists: {e}")
                return False

    def save_vod(self, data):
        with self.lock:
            try:
                self.cursor.execute('''
                INSERT OR REPLACE INTO vods (vod_id, vod_name, type_id, vod_pic, vod_remarks, vod_content, crawled_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    data.get('vod_id'),
                    data.get('vod_name'),
                    data.get('type_id'),
                    data.get('vod_pic'),
                    data.get('vod_remarks'),
                    data.get('vod_content'),
                    int(time.time())
                ))
                vod_pk = self.cursor.lastrowid
                if vod_pk == 0: 
                     self.cursor.execute('SELECT id FROM vods WHERE vod_id = ?', (data.get('vod_id'),))
                     res = self.cursor.fetchone()
                     if res: vod_pk = res[0]
                
                self.conn.commit()
                return vod_pk
            except Exception as e:
                print(f"[DB Error] save_vod: {e}")
                return None

    def save_episodes(self, vod_pk, episodes):
        # é¢„å¤„ç† source_id ä»¥å‡å°‘é”å†…æ“ä½œæ—¶é—´
        # ä½† get_or_create_source æœ¬èº«ä¹ŸåŠ é”ï¼Œæ‰€ä»¥è¿™é‡Œå¯ä»¥å…ˆæ”¶é›†
        processed_eps = []
        for ep in episodes:
            sid = self.get_or_create_source(ep['play_from'])
            processed_eps.append((sid, ep['name'], ep['url'], ep.get('resolved_url', '')))

        with self.lock:
            try:
                self.cursor.execute('DELETE FROM episodes WHERE vod_pk = ?', (vod_pk,))
                self.cursor.executemany('''
                INSERT INTO episodes (vod_pk, sid, name, raw_url, resolved_url)
                VALUES (?, ?, ?, ?, ?)
                ''', [(vod_pk, sid, name, raw_url, res_url) for sid, name, raw_url, res_url in processed_eps])
                self.conn.commit()
            except Exception as e:
                print(f"[DB Error] save_episodes: {e}")

    def close(self):
        self.conn.close()

# --- PHP æ¡¥æ¥è°ƒç”¨ (PHP Bridge) ---
class PHPBridge:
    def __init__(self, spider_path):
        self.spider_path = spider_path

    def call(self, method, *args):
        # æ„å»ºå‘½ä»¤
        cmd = [PHP_CMD, BRIDGE_SCRIPT, self.spider_path, method]
        cmd_args = []
        for arg in args:
            if isinstance(arg, (dict, list)):
                cmd_args.append(json.dumps(arg))
            else:
                cmd_args.append(str(arg))
        cmd.extend(cmd_args)
        
        try:
            # subprocess.run æ˜¯åŒæ­¥é˜»å¡çš„ï¼Œä½†åœ¨å¤šçº¿ç¨‹ä¸­è°ƒç”¨æ˜¯å®‰å…¨çš„
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
            if result.returncode != 0:
                if "Warning" not in result.stderr and "Notice" not in result.stderr:
                     pass 
                return None
            
            output = result.stdout.strip()
            try:
                json_res = json.loads(output)
                if json_res['status'] == 'success':
                    return json_res['data']
                else:
                    return None
            except json.JSONDecodeError:
                return None
                
        except Exception as e:
            print(f"[Bridge Error] {e}")
            return None

# --- ä»»åŠ¡è¿½è¸ªå™¨ (Task Tracker) ---
class TaskTracker:
    def __init__(self):
        self.lock = threading.Lock()
        self.cond = threading.Condition(self.lock)
        self.pending = 0

    def add(self, n=1):
        with self.lock:
            self.pending += n

    def done(self):
        with self.lock:
            self.pending -= 1
            if self.pending == 0:
                self.cond.notify_all()

    def wait_until_done(self):
        with self.lock:
            while self.pending > 0:
                self.cond.wait()

# --- ç»Ÿè®¡ä¸ç›‘æ§ (Stats & Monitor) ---
class Stats:
    def __init__(self):
        self.lock = threading.Lock()
        self.categories_found = 0
        self.pages_scanned = 0
        self.items_found = 0
        self.items_processed = 0
        self.items_skipped = 0
        self.episodes_resolved = 0
        self.errors = 0
        self.start_time = time.time()

    def inc(self, field, count=1):
        with self.lock:
            setattr(self, field, getattr(self, field) + count)

# --- çˆ¬è™«é€»è¾‘ (Crawler Logic) ---
class Crawler:
    def __init__(self, spider_path, db_path, max_pages=DEFAULT_MAX_PAGES, max_workers=DEFAULT_THREADS):
        self.spider_path = spider_path
        self.bridge = PHPBridge(spider_path)
        self.db = DBManager(db_path)
        self.max_pages = max_pages
        self.max_workers = max_workers
        self.stats = Stats()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.tracker = TaskTracker()
        self.running = True
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self.monitor_loop, daemon=True)
        self.monitor_thread.start()

    def submit_task(self, func, *args):
        self.tracker.add()
        self.executor.submit(self._wrap_task, func, *args)

    def _wrap_task(self, func, *args):
        try:
            func(*args)
        except Exception as e:
            print(f"[Task Error] {e}")
            self.stats.inc('errors')
        finally:
            self.tracker.done()

    def run(self):
        print(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å–: {os.path.basename(self.spider_path)}")
        print(f"âš™ï¸  é…ç½®: æœ€å¤§çº¿ç¨‹={self.max_workers}, æ¯ä¸ªåˆ†ç±»æœ€å¤§é¡µæ•°={self.max_pages}, è§£  æåœ°å€={RESOLVE_FINAL_URLS}")
        
        # 1. è·å–é¦–é¡µåˆ†ç±»
        home_data = self.bridge.call('homeContent', True)
        if not home_data or 'class' not in home_data:
            print("âŒ æ— æ³•è·å–åˆ†ç±»ä¿¡æ¯ï¼Œé€€å‡ºã€‚")
            return

        categories = home_data['class']
        self.stats.categories_found = len(categories)
        print(f"ğŸ“‹ è·å–åˆ° {len(categories)} ä¸ªåˆ†ç±»ï¼Œå¼€å§‹æ´¾å‘ä»»åŠ¡...")
        
        # 2. ä¿å­˜åˆ†ç±»å¹¶æ´¾å‘åˆ†ç±»ä»»åŠ¡
        for cat in categories:
            tid = str(cat['type_id'])
            name = cat['type_name']
            self.db.save_category(tid, name)
            self.submit_task(self.process_category, tid, name)
            
        # 3. ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        self.tracker.wait_until_done()
        self.running = False
        
        self.print_final_stats()
        
        # å…³é—­ executor å’Œ db
        self.executor.shutdown(wait=True)
        self.db.close()

    def monitor_loop(self):
        while self.running:
            self.print_progress()
            time.sleep(1)

    def print_progress(self):
        elapsed = time.time() - self.stats.start_time
        speed = self.stats.items_processed / elapsed if elapsed > 0 else 0
        # \033[K æ¸…é™¤å½“å‰è¡Œå‰©ä½™å†…å®¹ï¼Œç¡®ä¿æ›´æ–°æ—¶ä¸ä¼šæœ‰æ®‹ç•™å­—ç¬¦
        sys.stdout.write(
            f"\r\033[Kâ±ï¸  {elapsed:.1f}s | "
            f"Pages: {self.stats.pages_scanned} | "
            f"Items: {self.stats.items_processed}/{self.stats.items_found} | "
            f"Skip: {self.stats.items_skipped} | "
            f"Eps: {self.stats.episodes_resolved} | "
            f"Speed: {speed:.2f} it/s | "
            f"Err: {self.stats.errors}"
        )
        sys.stdout.flush()

    def print_final_stats(self):
        elapsed = time.time() - self.stats.start_time
        print("\n" + "-" * 50)
        print(f"ç»Ÿè®¡æŠ¥å‘Š:")
        print(f"  æ€»è€—æ—¶: {elapsed:.2f} ç§’")
        print(f"  æ‰«æé¡µæ•°: {self.stats.pages_scanned}")
        print(f"  å¤„ç†èµ„æº: {self.stats.items_processed}")
        print(f"  è·³è¿‡èµ„æº: {self.stats.items_skipped}")
        print(f"  è§£æé›†æ•°: {self.stats.episodes_resolved}")
        print(f"  é”™è¯¯æ•°é‡: {self.stats.errors}")
        print("-" * 50)

    def process_category(self, tid, tname):
        cat_data = self.bridge.call('categoryContent', tid, 1, False, {})
        
        if not cat_data or 'list' not in cat_data:
            self.stats.inc('errors')
            return

        items = cat_data.get('list', [])
        self.stats.inc('items_found', len(items))
        self.stats.inc('pages_scanned')
        
        for item in items:
            item['type_id'] = tid
            item['type_name'] = tname
            self.submit_task(self.process_item, item)

        page_count = 0
        if 'pagecount' in cat_data:
            try:
                page_count = int(cat_data['pagecount'])
            except:
                page_count = 9999
        
        # é€’å½’è§¦å‘ç¬¬2é¡µï¼ˆå¦‚æœéœ€è¦ï¼‰
        # å¦‚æœæ˜ç¡®è¿”å›åªæœ‰1é¡µï¼Œåˆ™åœæ­¢ï¼›å¦åˆ™åªè¦æ²¡è¾¾åˆ°max_pageså°±å°è¯•ä¸‹ä¸€é¡µ
        if page_count != 1:
            next_page = 2
            if not self.max_pages or next_page <= self.max_pages:
                self.submit_task(self.process_page, tid, tname, next_page)

    def process_page(self, tid, tname, page):
        cat_data = self.bridge.call('categoryContent', tid, page, False, {})
        if not cat_data or 'list' not in cat_data:
            self.stats.inc('errors')
            return
        
        items = cat_data.get('list', [])
        self.stats.inc('items_found', len(items))
        self.stats.inc('pages_scanned')
        
        if len(items) == 0:
            return

        for item in items:
            item['type_id'] = tid
            item['type_name'] = tname
            self.submit_task(self.process_item, item)

        # æäº¤ä¸‹ä¸€é¡µä»»åŠ¡ï¼ˆé€’å½’çˆ¬å–ï¼‰
        if len(items) > 0:
            next_page = page + 1
            if not self.max_pages or next_page <= self.max_pages:
                self.submit_task(self.process_page, tid, tname, next_page)

    def process_item(self, item):
        vod_id = item['vod_id']
        vod_name = item['vod_name']
        
        # å¢é‡çˆ¬å–æ£€æŸ¥ï¼šå¦‚æœæ•°æ®åº“ä¸­å·²å­˜åœ¨è¯¥ vod_idï¼Œåˆ™è·³è¿‡
        if self.db.item_exists(vod_id):
            # å³ä½¿è·³è¿‡ï¼Œä¹Ÿå¯ä»¥å°è¯•æ›´æ–° type_id (å¦‚æœä¹‹å‰ä¸ºç©º)
            # ä½†ä¸ºäº†æ€§èƒ½ï¼Œè¿™é‡Œæš‚æ—¶ç•¥è¿‡ï¼Œé™¤éå¼ºåˆ¶æ›´æ–°
            self.stats.inc('items_skipped')
            return
            
        # è¯¦æƒ…é¡µçˆ¬å–
        detail_res = self.bridge.call('detailContent', [vod_id])
        if not detail_res or 'list' not in detail_res or not detail_res['list']:
            self.stats.inc('errors')
            return
            
        vod_data = detail_res['list'][0]
        # è¡¥å…¨å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
        if 'vod_id' not in vod_data: vod_data['vod_id'] = vod_id
        if 'type_id' not in vod_data: vod_data['type_id'] = item.get('type_id')
        
        # å­˜å…¥ VOD ä¸»è¡¨
        vod_pk = self.db.save_vod(vod_data)
        if not vod_pk:
            self.stats.inc('errors')
            return

        self.stats.inc('items_processed')
        
        # å¤„ç†æ’­æ”¾åˆ—è¡¨
        play_from_str = vod_data.get('vod_play_from', '')
        play_url_str = vod_data.get('vod_play_url', '')
        
        if not play_from_str or not play_url_str:
            return
            
        play_from_list = play_from_str.split('$$$')
        play_url_list = play_url_str.split('$$$')
        
        all_episodes = []
        
        for i, source_name in enumerate(play_from_list):
            if i >= len(play_url_list): break
            url_text = play_url_list[i]
            
            # æ ¼å¼: åå­—$åœ°å€#åå­—$åœ°å€
            episodes = url_text.split('#')
            for ep_str in episodes:
                if '$' in ep_str:
                    ep_name, ep_url = ep_str.split('$', 1)
                else:
                    ep_name, ep_url = 'æ­£ç‰‡', ep_str
                    
                episode = {
                    'play_from': source_name,
                    'name': ep_name,
                    'url': ep_url,
                    'resolved_url': ''
                }
                
                if RESOLVE_FINAL_URLS:
                    play_res = self.bridge.call('playerContent', source_name, ep_url, [])
                    if play_res and 'url' in play_res:
                         episode['resolved_url'] = play_res['url']
                         self.stats.inc('episodes_resolved')
                    else:
                         pass
                
                all_episodes.append(episode)
        
        if all_episodes:
            self.db.save_episodes(vod_pk, all_episodes)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="DrPy PHP Spider Concurrent Crawler")
    parser.add_argument("spider", nargs="?", default=DEFAULT_SPIDER, help="PHP spider file path")
    parser.add_argument("-p", "--max-pages", type=int, default=DEFAULT_MAX_PAGES, help="Max pages per category")
    parser.add_argument("-t", "--threads", type=int, default=DEFAULT_THREADS, help="Concurrency threshold (max workers)")
    parser.add_argument("-n", "--no-resolve", action="store_true", help="Skip resolving final playback URLs")
    
    args = parser.parse_args()
    
    if args.no_resolve:
        RESOLVE_FINAL_URLS = False
        
    spider_file = args.spider
    if not os.path.exists(spider_file):
        print(f"Error: File not found: {spider_file}")
        sys.exit(1)
        
    # æ ¹æ®çˆ¬è™«æ–‡ä»¶åç”Ÿæˆæ•°æ®åº“æ–‡ä»¶å (ä¾‹å¦‚: spider.php -> spider.db)
    # ç¡®ä¿æ•°æ®åº“æ–‡ä»¶ç”Ÿæˆåœ¨çˆ¬è™«æ–‡ä»¶åŒçº§ç›®å½•
    spider_dir = os.path.dirname(os.path.abspath(spider_file))
    base_name = os.path.splitext(os.path.basename(spider_file))[0]
    db_path = os.path.join(spider_dir, f"{base_name}.db")
    
    print(f"ğŸ“ æ•°æ®åº“è·¯å¾„: {db_path}")

    crawler = Crawler(spider_file, db_path, args.max_pages, args.threads)
    crawler.run()
